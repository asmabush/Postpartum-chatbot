{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea28b21",
   "metadata": {},
   "source": [
    "A Rule-Based Chatbot for Postnatal Care Support \n",
    "NATURAL LANGUAGE PROCESSÄ°NG SEN534\n",
    "This chatbot was implemented using Google Collab\n",
    "created by: ASMA BUSHAALA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b2de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data\n",
    "with open('PostnatalCareData.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of intents or objects\n",
    "num_intents = len(data['ourIntents'])\n",
    "\n",
    "print(f\"Number of intent objects: {num_intents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ea734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "nltk.download('punkt_tab')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "all_words = []\n",
    "classes = []\n",
    "documents = []\n",
    "\n",
    "for intent in data['ourIntents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        tokens = nltk.word_tokenize(pattern)\n",
    "        all_words.extend(tokens)\n",
    "        documents.append((tokens, intent['tag']))\n",
    "    if intent['tag'] not in classes:\n",
    "        classes.append(intent['tag'])\n",
    "\n",
    "#lemmatize and lower words\n",
    "all_words = [lemmatizer.lemmatize(word.lower()) for word in all_words if word.isalpha()]\n",
    "all_words = sorted(set(all_words))\n",
    "classes = sorted(set(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b09a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words\n",
    "training_data = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = [lemmatizer.lemmatize(w.lower()) for w in doc[0]]\n",
    "    for word in all_words:\n",
    "        bag.append(1 if word in pattern_words else 0)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training_data.append([bag, output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.array(training_data, dtype=object)\n",
    "X = np.array(list(training_data[:, 0]))\n",
    "y = np.array(list(training_data[:, 1]))\n",
    "\n",
    "#Train - test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(X[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(y[0]), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Train\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=8, verbose=1)\n",
    "\n",
    "#save model and data ## i commented this because its additional step this is not required\n",
    "#model.save('chatbot_model.h5')\n",
    "#pickle.dump({'words': all_words, 'classes': classes, 'data': data}, open('chatbot_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1f3599",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "model = load_model('chatbot_model.h5')\n",
    "data = pickle.load(open('chatbot_data.pkl', 'rb'))\n",
    "\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "intents = data['data']['ourIntents']\n",
    "\n",
    "def clean_input(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "def bag_of_words(sentence, words):\n",
    "    tokens = clean_input(sentence)\n",
    "    bag = [0] * len(words)\n",
    "    for token in tokens:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == token:\n",
    "                bag[i] = 1\n",
    "    return np.array(bag)\n",
    "\n",
    "def predict_class(sentence):\n",
    "    bow = bag_of_words(sentence, words)\n",
    "    result = model.predict(np.array([bow]))[0]\n",
    "    threshold = 0.6 \n",
    "    if max(result) < threshold:\n",
    "        return None\n",
    "    return classes[np.argmax(result)]\n",
    "\n",
    "def get_response(intent_tag):\n",
    "    for intent in intents:\n",
    "        if intent['tag'] == intent_tag:\n",
    "            return random.choice(intent['responses'])\n",
    "    return \"Sorry, I didn't understand that.\"\n",
    "\n",
    "total_inputs = 0\n",
    "matched_inputs = 0\n",
    "fallback_count = 0\n",
    "\n",
    "##chatting loop \n",
    "total_inputs = 0\n",
    "matched_inputs = 0\n",
    "fallback_count = 0\n",
    "response_times = []\n",
    "\n",
    "print(\"ðŸ‘¶ Postnatal Care Chatbot is ready! Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip().lower()\n",
    "\n",
    "    if user_input in [\"quit\", \"exit\", \"stop\", \"bye\",\"ok bye\"]:\n",
    "        print(\"Bot: Thanks for chatting, mama! Take care ðŸ’œ\")\n",
    "        break\n",
    "\n",
    "    total_inputs += 1\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    intent_tag = predict_class(user_input)\n",
    "\n",
    "    if intent_tag:\n",
    "        matched_inputs += 1\n",
    "        response = get_response(intent_tag)\n",
    "    else:\n",
    "        fallback_count += 1\n",
    "        response = (\"Sorry, I didnâ€™t understand. \"\n",
    "                    \"You can ask about baby sleep, feeding, recovery, or mental health.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    response_times.append(elapsed)\n",
    "\n",
    "    print(f\"Bot: {response}\")\n",
    "    print(f\"(â± Response time: {elapsed:.2f} seconds)\")\n",
    "\n",
    "#after chat ends display session summary (evaluation)\n",
    "if total_inputs > 0:\n",
    "    intent_match_rate = (matched_inputs / total_inputs) * 100\n",
    "    fallback_rate = (fallback_count / total_inputs) * 100\n",
    "    avg_response_time = sum(response_times) / len(response_times)\n",
    "\n",
    "    print(\"\\nðŸ“Š Session Summary:\")\n",
    "    print(f\"Total messages: {total_inputs}\")\n",
    "    print(f\"Matched intents: {matched_inputs}\")\n",
    "    print(f\"Fallbacks: {fallback_count}\")\n",
    "    print(f\"Intent Match Rate: {intent_match_rate:.2f}%\")\n",
    "    print(f\"Fallback Rate: {fallback_rate:.2f}%\")\n",
    "    print(f\"Average Response Time: {avg_response_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
